---
phase: 02-data-source-mcp
plan: 04
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/mcp/data_source/adapters/db_adapter.py
  - src/mcp/data_source/tools/import_tools.py
  - src/mcp/data_source/server.py
autonomous: true

must_haves:
  truths:
    - "User can list tables from a PostgreSQL database"
    - "User can list tables from a MySQL database"
    - "User can import data from a database table with a query"
    - "Large tables (>10k rows) require WHERE clause"
    - "Connection strings are not stored after import"
  artifacts:
    - path: "src/mcp/data_source/adapters/db_adapter.py"
      provides: "Database import via DuckDB postgres/mysql extensions"
      exports: ["DatabaseAdapter"]
    - path: "src/mcp/data_source/tools/import_tools.py"
      provides: "MCP import_database and list_tables tools"
      exports: ["import_database", "list_tables"]
  key_links:
    - from: "src/mcp/data_source/adapters/db_adapter.py"
      to: "DuckDB postgres extension"
      via: "ATTACH/DETACH"
      pattern: "ATTACH.*TYPE postgres"
    - from: "src/mcp/data_source/adapters/db_adapter.py"
      to: "DuckDB mysql extension"
      via: "ATTACH/DETACH"
      pattern: "ATTACH.*TYPE mysql"
---

<objective>
Implement database import capability for PostgreSQL and MySQL with snapshot semantics and large table protection.

Purpose: Satisfy DATA-03 requirement - users can import shipment data from database (Postgres/MySQL) via connection string.

Output: Working import_database and list_tables MCP tools with security-conscious design.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-source-mcp/02-CONTEXT.md
@.planning/phases/02-data-source-mcp/02-RESEARCH.md
@.planning/phases/02-data-source-mcp/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database Adapter Implementation</name>
  <files>
    src/mcp/data_source/adapters/db_adapter.py
    src/mcp/data_source/adapters/__init__.py
  </files>
  <action>
Create the database adapter using DuckDB's postgres and mysql extensions.

1. Create `src/mcp/data_source/adapters/db_adapter.py`:
   ```python
   """Database adapter for Data Source MCP."""

   import re
   from typing import Literal
   from urllib.parse import urlparse
   from duckdb import DuckDBPyConnection

   from .base import BaseSourceAdapter
   from ..models import ImportResult, SchemaColumn


   # Threshold for requiring WHERE clause (per CONTEXT.md)
   LARGE_TABLE_THRESHOLD = 10000


   class DatabaseAdapter(BaseSourceAdapter):
       """Adapter for importing from PostgreSQL and MySQL databases via DuckDB."""

       @property
       def source_type(self) -> str:
           return "database"

       def _detect_db_type(self, connection_string: str) -> Literal["postgres", "mysql"]:
           """Detect database type from connection string.

           Args:
               connection_string: Database connection URL

           Returns:
               "postgres" or "mysql"

           Raises:
               ValueError: If database type not supported
           """
           parsed = urlparse(connection_string)
           scheme = parsed.scheme.lower()

           if scheme in ("postgresql", "postgres"):
               return "postgres"
           elif scheme == "mysql":
               return "mysql"
           else:
               raise ValueError(
                   f"Unsupported database type: {scheme}. "
                   "Supported: postgresql://, postgres://, mysql://"
               )

       def list_tables(
           self,
           conn: DuckDBPyConnection,
           connection_string: str,
           schema: str = "public",
       ) -> list[dict]:
           """List tables in the remote database.

           Args:
               conn: DuckDB connection
               connection_string: Database connection URL
               schema: Schema to list tables from (default: public for Postgres)

           Returns:
               List of dicts with table name and row count estimate
           """
           db_type = self._detect_db_type(connection_string)

           # Attach database temporarily
           conn.execute(f"ATTACH '{connection_string}' AS remote_db (TYPE {db_type}, READ_ONLY)")

           try:
               if db_type == "postgres":
                   # Get tables from information_schema
                   tables = conn.execute(f"""
                       SELECT table_name
                       FROM remote_db.information_schema.tables
                       WHERE table_schema = '{schema}'
                       AND table_type = 'BASE TABLE'
                       ORDER BY table_name
                   """).fetchall()
               else:  # mysql
                   # MySQL uses different metadata queries
                   tables = conn.execute("""
                       SELECT table_name
                       FROM remote_db.information_schema.tables
                       WHERE table_schema = DATABASE()
                       AND table_type = 'BASE TABLE'
                       ORDER BY table_name
                   """).fetchall()

               result = []
               for (table_name,) in tables:
                   # Get approximate row count
                   try:
                       count = conn.execute(f"""
                           SELECT COUNT(*) FROM remote_db.{schema}.{table_name}
                       """).fetchone()[0]
                   except Exception:
                       count = None

                   result.append({
                       "name": table_name,
                       "row_count": count,
                       "requires_filter": count is not None and count > LARGE_TABLE_THRESHOLD
                   })

               return result

           finally:
               # Always detach to avoid storing connection
               conn.execute("DETACH remote_db")

       def import_data(
           self,
           conn: DuckDBPyConnection,
           connection_string: str,
           query: str,
           schema: str = "public",
       ) -> ImportResult:
           """Import data from database using a query.

           Per CONTEXT.md: Snapshot on import - query runs once, results cached locally.
           Connection string is NOT stored after import.

           Args:
               conn: DuckDB connection
               connection_string: Database connection URL
               query: SQL query to execute (must include WHERE for large tables)
               schema: Schema name (default: public)

           Returns:
               ImportResult with schema and row count

           Raises:
               ValueError: If query targets large table without WHERE clause
           """
           db_type = self._detect_db_type(connection_string)

           # Attach database temporarily
           conn.execute(f"ATTACH '{connection_string}' AS remote_db (TYPE {db_type}, READ_ONLY)")

           try:
               # Extract table name from query for validation
               # Simple regex - handles "FROM table" and "FROM schema.table"
               table_match = re.search(r'FROM\s+(\w+\.)?(\w+)', query, re.IGNORECASE)
               if table_match:
                   table_name = table_match.group(2)

                   # Check if large table without filter
                   has_where = re.search(r'\bWHERE\b', query, re.IGNORECASE) is not None

                   if not has_where:
                       # Check row count
                       try:
                           count_result = conn.execute(f"""
                               SELECT COUNT(*) FROM remote_db.{schema}.{table_name}
                           """).fetchone()
                           row_count = count_result[0] if count_result else 0

                           if row_count > LARGE_TABLE_THRESHOLD:
                               raise ValueError(
                                   f"Table '{table_name}' has {row_count:,} rows. "
                                   f"Add a WHERE clause to filter (tables > {LARGE_TABLE_THRESHOLD:,} rows require filters). "
                                   f"Example: SELECT * FROM {table_name} WHERE created_at > '2026-01-01'"
                               )
                       except Exception as e:
                           if "rows" in str(e):
                               raise  # Re-raise our validation error
                           # Ignore other errors (table might not exist yet in query)

               # Rewrite query to use remote_db prefix
               # Add remote_db. prefix to table references
               modified_query = re.sub(
                   r'FROM\s+(\w+)(\s|$)',
                   f'FROM remote_db.{schema}.\\1\\2',
                   query,
                   flags=re.IGNORECASE
               )

               # Create snapshot table
               conn.execute(f"""
                   CREATE OR REPLACE TABLE imported_data AS
                   {modified_query}
               """)

               # Get schema
               schema_rows = conn.execute("DESCRIBE imported_data").fetchall()
               columns = [
                   SchemaColumn(
                       name=col[0],
                       type=col[1],
                       nullable=col[2] == "YES",
                       warnings=[]
                   )
                   for col in schema_rows
               ]

               # Get row count
               row_count = conn.execute("SELECT COUNT(*) FROM imported_data").fetchone()[0]

               return ImportResult(
                   row_count=row_count,
                   columns=columns,
                   warnings=[],
                   source_type="database"
               )

           finally:
               # Always detach - connection string is NOT stored
               conn.execute("DETACH remote_db")

       def get_metadata(self, conn: DuckDBPyConnection) -> dict:
           """Get metadata about imported database data."""
           try:
               row_count = conn.execute("SELECT COUNT(*) FROM imported_data").fetchone()[0]
               columns = conn.execute("DESCRIBE imported_data").fetchall()
               return {
                   "row_count": row_count,
                   "column_count": len(columns),
                   "source_type": "database"
               }
           except Exception:
               return {"error": "No data imported"}
   ```

2. Update `src/mcp/data_source/adapters/__init__.py` to export DatabaseAdapter.
  </action>
  <verify>
    - `python -c "from src.mcp.data_source.adapters.db_adapter import DatabaseAdapter"` succeeds
    - DatabaseAdapter().source_type returns "database"
    - _detect_db_type correctly identifies postgres and mysql
  </verify>
  <done>DatabaseAdapter class created with list_tables, import_data, and get_metadata methods.</done>
</task>

<task type="auto">
  <name>Task 2: import_database and list_tables MCP Tools</name>
  <files>
    src/mcp/data_source/tools/import_tools.py
    src/mcp/data_source/server.py
  </files>
  <action>
Add import_database and list_tables tools to the MCP server.

1. Update `src/mcp/data_source/tools/import_tools.py` to add:
   ```python
   from ..adapters.db_adapter import DatabaseAdapter


   async def list_tables(
       connection_string: str,
       ctx: Context,
       schema: str = "public",
   ) -> dict:
       """List tables in a remote database.

       Args:
           connection_string: Database connection URL
               - PostgreSQL: postgresql://user:pass@host:5432/dbname
               - MySQL: mysql://user:pass@host:3306/dbname
           schema: Schema to list tables from (default: public)

       Returns:
           Dictionary with list of tables and their row counts.
           Tables > 10,000 rows are flagged as requiring a WHERE clause.

       Security: Connection string is used only during this call and NOT stored.

       Example:
           >>> result = await list_tables("postgresql://user:pass@localhost/orders", ctx)
           >>> print(result["tables"])
           [{"name": "orders", "row_count": 50000, "requires_filter": True}, ...]
       """
       db = ctx.request_context.lifespan_context["db"]

       await ctx.info(f"Listing tables from database")  # Don't log connection string!

       adapter = DatabaseAdapter()
       tables = adapter.list_tables(conn=db, connection_string=connection_string, schema=schema)

       await ctx.info(f"Found {len(tables)} tables")

       return {"tables": tables, "count": len(tables), "schema": schema}


   async def import_database(
       connection_string: str,
       query: str,
       ctx: Context,
       schema: str = "public",
   ) -> dict:
       """Import data from a database using a SQL query.

       Creates a snapshot of the query results - the database is NOT kept connected.

       Args:
           connection_string: Database connection URL
               - PostgreSQL: postgresql://user:pass@host:5432/dbname
               - MySQL: mysql://user:pass@host:3306/dbname
           query: SQL SELECT query to execute
           schema: Schema name for table references (default: public)

       Returns:
           Dictionary with schema, row_count, and any warnings.

       Security:
           - Connection string is used only during import and NOT stored
           - Query is executed read-only
           - Tables > 10,000 rows require a WHERE clause

       Example:
           >>> result = await import_database(
           ...     "postgresql://user:pass@localhost/shipping",
           ...     "SELECT * FROM orders WHERE created_at > '2026-01-01'",
           ...     ctx
           ... )
           >>> print(result["row_count"])
           1500
       """
       db = ctx.request_context.lifespan_context["db"]

       await ctx.info("Importing from database")  # Don't log connection string!

       adapter = DatabaseAdapter()
       result = adapter.import_data(
           conn=db,
           connection_string=connection_string,
           query=query,
           schema=schema,
       )

       # Update current source tracking (without connection string!)
       ctx.request_context.lifespan_context["current_source"] = {
           "type": "database",
           "query": query,
           "row_count": result.row_count,
       }

       await ctx.info(f"Imported {result.row_count} rows with {len(result.columns)} columns")

       return result.model_dump()
   ```

2. Update `src/mcp/data_source/tools/__init__.py` to export new tools.

3. Update `src/mcp/data_source/server.py` to register new tools:
   ```python
   from .tools.import_tools import (
       import_csv,
       import_excel,
       list_sheets,
       import_database,
       list_tables,
   )

   mcp.tool()(import_csv)
   mcp.tool()(import_excel)
   mcp.tool()(list_sheets)
   mcp.tool()(import_database)
   mcp.tool()(list_tables)
   ```

CRITICAL: NEVER log connection strings - they contain credentials!
  </action>
  <verify>
    - `python -c "from src.mcp.data_source.server import mcp; print([t.name for t in mcp.list_tools()])"` shows import_database and list_tables
  </verify>
  <done>import_database and list_tables tools registered with FastMCP server.</done>
</task>

<task type="auto">
  <name>Task 3: Database Adapter Unit Tests</name>
  <files>
    tests/mcp/test_db_adapter.py
  </files>
  <action>
Create unit tests for database adapter logic. Note: These test the adapter logic, not actual database connections (which would require test databases).

1. Create `tests/mcp/test_db_adapter.py`:
   ```python
   """Test database adapter functionality."""

   import pytest
   import duckdb

   from src.mcp.data_source.adapters.db_adapter import DatabaseAdapter, LARGE_TABLE_THRESHOLD


   @pytest.fixture
   def adapter():
       return DatabaseAdapter()


   @pytest.fixture
   def duckdb_conn():
       """Create in-memory DuckDB connection with extensions."""
       conn = duckdb.connect(":memory:")
       # Note: postgres/mysql extensions require actual databases to test fully
       yield conn
       conn.close()


   class TestDatabaseTypeDetection:
       """Test database type detection from connection strings."""

       def test_detect_postgres(self, adapter):
           assert adapter._detect_db_type("postgresql://user:pass@host/db") == "postgres"
           assert adapter._detect_db_type("postgres://user:pass@host/db") == "postgres"

       def test_detect_mysql(self, adapter):
           assert adapter._detect_db_type("mysql://user:pass@host/db") == "mysql"

       def test_unsupported_type(self, adapter):
           with pytest.raises(ValueError) as exc_info:
               adapter._detect_db_type("sqlite:///path/to/db.sqlite")
           assert "Unsupported database type" in str(exc_info.value)

       def test_invalid_url(self, adapter):
           # urlparse handles this gracefully
           with pytest.raises(ValueError):
               adapter._detect_db_type("not-a-valid-url")


   class TestLargeTableProtection:
       """Test large table threshold validation."""

       def test_threshold_value(self):
           assert LARGE_TABLE_THRESHOLD == 10000

       def test_source_type(self, adapter):
           assert adapter.source_type == "database"


   class TestQueryRewriting:
       """Test that queries are properly modified for remote_db prefix.

       Note: Full integration tests require actual database connections.
       These tests verify the adapter initializes correctly.
       """

       def test_adapter_instantiation(self, adapter):
           """Verify adapter can be created."""
           assert adapter is not None
           assert adapter.source_type == "database"

       def test_get_metadata_no_data(self, adapter, duckdb_conn):
           """Test get_metadata when no data imported."""
           result = adapter.get_metadata(duckdb_conn)
           assert "error" in result
   ```

2. Run tests: `pytest tests/mcp/test_db_adapter.py -v`

NOTE: Full database integration tests would require:
- Running PostgreSQL and MySQL test containers
- Creating test data
- This is better suited for CI/CD with docker-compose

For now, we test the logic that doesn't require database connections.
  </action>
  <verify>
    - `pytest tests/mcp/test_db_adapter.py -v` passes all tests
    - Database type detection works correctly
    - LARGE_TABLE_THRESHOLD is 10000
  </verify>
  <done>Database adapter unit tests passing. Full integration tests deferred to CI with test databases.</done>
</task>

</tasks>

<verification>
Run these commands to verify plan completion:

```bash
cd /Users/matthewhans/Desktop/Programming/ShipAgent
source .venv/bin/activate

# Run database adapter tests
pytest tests/mcp/test_db_adapter.py -v

# Verify tools are registered
python -c "
from src.mcp.data_source.server import mcp
tools = mcp.list_tools()
tool_names = [t.name for t in tools]
assert 'import_database' in tool_names, f'import_database not found in {tool_names}'
assert 'list_tables' in tool_names, f'list_tables not found in {tool_names}'
print('Database tools registered successfully')
"

# Test database type detection
python -c "
from src.mcp.data_source.adapters.db_adapter import DatabaseAdapter

adapter = DatabaseAdapter()
assert adapter._detect_db_type('postgresql://u:p@h/d') == 'postgres'
assert adapter._detect_db_type('mysql://u:p@h/d') == 'mysql'

try:
    adapter._detect_db_type('sqlite:///db.sqlite')
    print('ERROR: Should have raised ValueError')
except ValueError as e:
    print(f'Correctly rejected sqlite: {e}')

print('Database type detection working')
"
```
</verification>

<success_criteria>
1. DatabaseAdapter class implements BaseSourceAdapter interface
2. list_tables returns table names with row counts and requires_filter flag
3. import_database creates snapshot from query results
4. Large tables (>10k rows) without WHERE clause are rejected with helpful error
5. Connection strings are NEVER logged or stored
6. Both postgres:// and mysql:// connection strings supported
7. All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-source-mcp/02-04-SUMMARY.md`
</output>
