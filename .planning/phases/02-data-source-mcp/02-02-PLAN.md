---
phase: 02-data-source-mcp
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/mcp/data_source/adapters/csv_adapter.py
  - src/mcp/data_source/tools/import_tools.py
  - src/mcp/data_source/server.py
autonomous: true

must_haves:
  truths:
    - "User can import a CSV file and see discovered schema"
    - "Mixed-type columns default to string"
    - "Ambiguous dates generate warnings with US/EU interpretations"
    - "Empty rows are silently skipped"
  artifacts:
    - path: "src/mcp/data_source/adapters/csv_adapter.py"
      provides: "CSV import via DuckDB read_csv"
      exports: ["CSVAdapter"]
    - path: "src/mcp/data_source/tools/import_tools.py"
      provides: "MCP import_csv tool"
      exports: ["import_csv"]
  key_links:
    - from: "src/mcp/data_source/tools/import_tools.py"
      to: "CSVAdapter"
      via: "adapter instantiation"
      pattern: "CSVAdapter\\("
    - from: "src/mcp/data_source/server.py"
      to: "import_csv tool"
      via: "@mcp.tool decorator"
      pattern: "@mcp\\.tool"
---

<objective>
Implement CSV import capability with automatic schema discovery, mixed-type handling, and date ambiguity detection.

Purpose: Satisfy DATA-01 requirement - users can import shipment data from CSV files with automatic schema discovery.

Output: Working import_csv MCP tool that loads CSV into DuckDB and returns discovered schema.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-source-mcp/02-CONTEXT.md
@.planning/phases/02-data-source-mcp/02-RESEARCH.md
@.planning/phases/02-data-source-mcp/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: CSV Adapter Implementation</name>
  <files>
    src/mcp/data_source/adapters/csv_adapter.py
    src/mcp/data_source/adapters/__init__.py
  </files>
  <action>
Create the CSV adapter using DuckDB's read_csv with proper error handling.

1. Create `src/mcp/data_source/adapters/csv_adapter.py`:
   ```python
   from pathlib import Path
   from duckdb import DuckDBPyConnection

   from .base import BaseSourceAdapter
   from ..models import ImportResult, SchemaColumn
   from ..utils import parse_date_with_warnings

   class CSVAdapter(BaseSourceAdapter):
       """Adapter for importing CSV files via DuckDB."""

       @property
       def source_type(self) -> str:
           return "csv"

       def import_data(
           self,
           conn: DuckDBPyConnection,
           file_path: str,
           delimiter: str = ",",
           header: bool = True,
       ) -> ImportResult:
           """Import CSV file into DuckDB.

           Args:
               conn: DuckDB connection
               file_path: Path to CSV file
               delimiter: Column delimiter (default comma)
               header: Whether first row is header (default True)

           Returns:
               ImportResult with schema and row count
           """
           # Validate file exists
           path = Path(file_path)
           if not path.exists():
               raise FileNotFoundError(f"CSV file not found: {file_path}")

           # Import with auto-detection, full file scan for type inference
           # Per RESEARCH.md: sample_size=-1, ignore_errors=true for mixed types
           conn.execute(f"""
               CREATE OR REPLACE TABLE imported_data AS
               SELECT * FROM read_csv(
                   '{file_path}',
                   auto_detect = true,
                   sample_size = -1,
                   ignore_errors = true,
                   null_padding = true,
                   delim = '{delimiter}',
                   header = {str(header).lower()}
               )
           """)

           # Get schema
           schema_rows = conn.execute("DESCRIBE imported_data").fetchall()
           columns = []
           warnings = []

           for col_name, col_type, nullable, key, default, extra in schema_rows:
               col_warnings = []

               # Check for date columns that might have ambiguity
               if "DATE" in col_type.upper() or "TIMESTAMP" in col_type.upper():
                   # Sample a value to check for ambiguity
                   sample = conn.execute(f"""
                       SELECT "{col_name}" FROM imported_data
                       WHERE "{col_name}" IS NOT NULL LIMIT 1
                   """).fetchone()
                   if sample:
                       date_result = parse_date_with_warnings(str(sample[0]))
                       col_warnings.extend(date_result.get("warnings", []))

               columns.append(SchemaColumn(
                   name=col_name,
                   type=col_type,
                   nullable=nullable == "YES",
                   warnings=[w.get("message", str(w)) if isinstance(w, dict) else str(w) for w in col_warnings]
               ))
               warnings.extend(col_warnings)

           # Get row count (excluding empty rows - DuckDB handles this)
           row_count = conn.execute("SELECT COUNT(*) FROM imported_data").fetchone()[0]

           return ImportResult(
               row_count=row_count,
               columns=columns,
               warnings=[w.get("message", str(w)) if isinstance(w, dict) else str(w) for w in warnings],
               source_type="csv"
           )

       def get_metadata(self, conn: DuckDBPyConnection) -> dict:
           """Get metadata about imported CSV."""
           try:
               row_count = conn.execute("SELECT COUNT(*) FROM imported_data").fetchone()[0]
               columns = conn.execute("DESCRIBE imported_data").fetchall()
               return {
                   "row_count": row_count,
                   "column_count": len(columns),
                   "source_type": "csv"
               }
           except Exception:
               return {"error": "No data imported"}
   ```

2. Update `src/mcp/data_source/adapters/__init__.py` to export CSVAdapter.
  </action>
  <verify>
    - `python -c "from src.mcp.data_source.adapters.csv_adapter import CSVAdapter"` succeeds
    - CSVAdapter().source_type returns "csv"
  </verify>
  <done>CSVAdapter class created with import_data and get_metadata methods.</done>
</task>

<task type="auto">
  <name>Task 2: import_csv MCP Tool</name>
  <files>
    src/mcp/data_source/tools/import_tools.py
    src/mcp/data_source/tools/__init__.py
    src/mcp/data_source/server.py
  </files>
  <action>
Create the import_csv MCP tool and register it with the server.

1. Create `src/mcp/data_source/tools/import_tools.py`:
   ```python
   """Import tools for Data Source MCP."""

   from fastmcp import Context

   from ..adapters.csv_adapter import CSVAdapter
   from ..models import ImportResult


   async def import_csv(
       file_path: str,
       ctx: Context,
       delimiter: str = ",",
       header: bool = True,
   ) -> dict:
       """Import CSV file and discover schema.

       Args:
           file_path: Absolute path to the CSV file
           delimiter: Column delimiter (default: comma)
           header: Whether first row contains headers (default: True)

       Returns:
           Dictionary with schema, row_count, and any warnings about data types.

       Example:
           >>> result = await import_csv("/path/to/orders.csv", ctx)
           >>> print(result["row_count"])
           150
           >>> print(result["columns"][0])
           {"name": "order_id", "type": "INTEGER", "nullable": false}
       """
       db = ctx.request_context.lifespan_context["db"]

       await ctx.info(f"Importing CSV from {file_path}")

       adapter = CSVAdapter()
       result = adapter.import_data(
           conn=db,
           file_path=file_path,
           delimiter=delimiter,
           header=header,
       )

       # Update current source tracking
       ctx.request_context.lifespan_context["current_source"] = {
           "type": "csv",
           "path": file_path,
           "row_count": result.row_count,
       }

       await ctx.info(f"Imported {result.row_count} rows with {len(result.columns)} columns")

       return result.model_dump()
   ```

2. Update `src/mcp/data_source/tools/__init__.py` to export import_csv.

3. Update `src/mcp/data_source/server.py` to register the tool:
   ```python
   from contextlib import asynccontextmanager
   from fastmcp import FastMCP, Context
   import duckdb

   @asynccontextmanager
   async def lifespan(app):
       """Initialize DuckDB with database extensions."""
       conn = duckdb.connect(":memory:")
       conn.execute("INSTALL postgres; INSTALL mysql;")
       conn.execute("LOAD postgres; LOAD mysql;")

       yield {
           "db": conn,
           "current_source": None,
           "type_overrides": {},
       }

       conn.close()

   mcp = FastMCP("DataSource", lifespan=lifespan)

   # Import and register tools
   from .tools.import_tools import import_csv

   # Register as MCP tool
   mcp.tool()(import_csv)

   if __name__ == "__main__":
       mcp.run(transport="stdio")
   ```

CRITICAL: Use ctx.request_context.lifespan_context to access the lifespan context in tools. Do NOT use ctx.lifespan_context directly.
  </action>
  <verify>
    - `python -c "from src.mcp.data_source.server import mcp; print([t.name for t in mcp.list_tools()])"` shows import_csv
  </verify>
  <done>import_csv tool registered with FastMCP server.</done>
</task>

<task type="auto">
  <name>Task 3: Integration Test with Sample CSV</name>
  <files>
    tests/mcp/test_csv_import.py
    tests/mcp/__init__.py
    tests/__init__.py
  </files>
  <action>
Create a test to verify CSV import works end-to-end.

1. Create test directories if needed: `tests/`, `tests/mcp/`

2. Create `tests/mcp/test_csv_import.py`:
   ```python
   """Test CSV import functionality."""

   import tempfile
   import pytest
   import duckdb

   from src.mcp.data_source.adapters.csv_adapter import CSVAdapter
   from src.mcp.data_source.models import ImportResult


   @pytest.fixture
   def sample_csv():
       """Create a temporary CSV file for testing."""
       content = """order_id,customer_name,ship_date,weight,state
   1001,John Doe,2026-01-15,5.5,CA
   1002,Jane Smith,01/20/2026,3.2,NY
   1003,Bob Wilson,2026-01-25,7.8,TX
   ,,,,
   1004,Alice Brown,2026-01-30,2.1,FL"""

       with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
           f.write(content)
           return f.name


   @pytest.fixture
   def duckdb_conn():
       """Create in-memory DuckDB connection."""
       conn = duckdb.connect(":memory:")
       yield conn
       conn.close()


   def test_csv_import_basic(sample_csv, duckdb_conn):
       """Test basic CSV import with schema discovery."""
       adapter = CSVAdapter()
       result = adapter.import_data(duckdb_conn, sample_csv)

       assert isinstance(result, ImportResult)
       assert result.row_count == 4  # Empty row skipped
       assert result.source_type == "csv"
       assert len(result.columns) == 5

       # Check column names
       col_names = [c.name for c in result.columns]
       assert "order_id" in col_names
       assert "customer_name" in col_names
       assert "ship_date" in col_names


   def test_csv_import_file_not_found(duckdb_conn):
       """Test error handling for missing file."""
       adapter = CSVAdapter()
       with pytest.raises(FileNotFoundError):
           adapter.import_data(duckdb_conn, "/nonexistent/file.csv")


   def test_csv_mixed_types(duckdb_conn):
       """Test that mixed-type columns default to string."""
       content = """id,value
   1,100
   2,abc
   3,200"""

       with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
           f.write(content)
           csv_path = f.name

       adapter = CSVAdapter()
       result = adapter.import_data(duckdb_conn, csv_path)

       # The 'value' column should be VARCHAR due to mixed types
       value_col = next(c for c in result.columns if c.name == "value")
       assert "VARCHAR" in value_col.type.upper()
   ```

3. Run tests: `pytest tests/mcp/test_csv_import.py -v`
  </action>
  <verify>
    - `pytest tests/mcp/test_csv_import.py -v` passes all tests
    - Empty rows are skipped (row count is 4, not 5)
    - Mixed-type column detected as VARCHAR
  </verify>
  <done>CSV import tested: schema discovery, empty row handling, mixed types all verified.</done>
</task>

</tasks>

<verification>
Run these commands to verify plan completion:

```bash
cd /Users/matthewhans/Desktop/Programming/ShipAgent
source .venv/bin/activate

# Run CSV import tests
pytest tests/mcp/test_csv_import.py -v

# Verify tool is registered
python -c "
from src.mcp.data_source.server import mcp
tools = mcp.list_tools()
tool_names = [t.name for t in tools]
assert 'import_csv' in tool_names, f'import_csv not found in {tool_names}'
print('import_csv tool registered successfully')
"

# Test with a real CSV
python -c "
import tempfile
import duckdb
from src.mcp.data_source.adapters.csv_adapter import CSVAdapter

# Create test CSV
csv_content = '''name,city,state
Alice,Los Angeles,CA
Bob,New York,NY
Charlie,Houston,TX'''

with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
    f.write(csv_content)
    csv_path = f.name

conn = duckdb.connect(':memory:')
adapter = CSVAdapter()
result = adapter.import_data(conn, csv_path)

print(f'Imported {result.row_count} rows')
print(f'Columns: {[c.name for c in result.columns]}')
assert result.row_count == 3
print('CSV import working correctly')
"
```
</verification>

<success_criteria>
1. CSVAdapter class implements BaseSourceAdapter interface
2. import_csv MCP tool registered and callable
3. CSV files import into DuckDB with auto-detected schema
4. Empty rows silently skipped (not counted)
5. Mixed-type columns default to VARCHAR
6. Date columns with ambiguous formats generate warnings
7. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-source-mcp/02-02-SUMMARY.md`
</output>
