---
phase: 02-data-source-mcp
plan: 05
type: execute
wave: 3
depends_on: ["02-02", "02-03", "02-04"]
files_modified:
  - src/mcp/data_source/tools/schema_tools.py
  - src/mcp/data_source/tools/query_tools.py
  - src/mcp/data_source/tools/checksum_tools.py
  - src/mcp/data_source/server.py
autonomous: true

must_haves:
  truths:
    - "User can get schema of imported data"
    - "User can override column type after import"
    - "User can query rows by filter"
    - "User can get individual row by number"
    - "Each row has a SHA-256 checksum"
    - "Checksums are deterministic (same data = same checksum)"
  artifacts:
    - path: "src/mcp/data_source/tools/schema_tools.py"
      provides: "get_schema, override_column_type tools"
      exports: ["get_schema", "override_column_type"]
    - path: "src/mcp/data_source/tools/query_tools.py"
      provides: "query_data, get_row, get_rows_by_filter tools"
      exports: ["query_data", "get_row", "get_rows_by_filter"]
    - path: "src/mcp/data_source/tools/checksum_tools.py"
      provides: "compute_checksums, verify_checksum tools"
      exports: ["compute_checksums", "verify_checksum"]
  key_links:
    - from: "src/mcp/data_source/tools/checksum_tools.py"
      to: "utils.compute_row_checksum"
      via: "import"
      pattern: "from.*utils import.*compute_row_checksum"
    - from: "src/mcp/data_source/tools/query_tools.py"
      to: "DuckDB imported_data table"
      via: "SQL queries"
      pattern: "SELECT.*FROM imported_data"
---

<objective>
Implement schema inspection, data querying, and checksum computation tools.

Purpose: Satisfy DATA-05 requirement (row checksums) and provide the data access layer the orchestrator needs for batch processing.

Output: Complete set of MCP tools for schema inspection, data access, and integrity verification.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-source-mcp/02-CONTEXT.md
@.planning/phases/02-data-source-mcp/02-RESEARCH.md
@.planning/phases/02-data-source-mcp/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Schema Tools</name>
  <files>
    src/mcp/data_source/tools/schema_tools.py
    src/mcp/data_source/tools/__init__.py
    src/mcp/data_source/server.py
  </files>
  <action>
Create tools for schema inspection and type override.

1. Create `src/mcp/data_source/tools/schema_tools.py`:
   ```python
   """Schema inspection and modification tools for Data Source MCP."""

   from fastmcp import Context

   from ..models import SchemaColumn


   async def get_schema(ctx: Context) -> dict:
       """Get schema of currently imported data.

       Returns the column names, types, and any warnings about data quality
       (such as ambiguous date formats or mixed types).

       Returns:
           Dictionary with columns list and source metadata.

       Raises:
           ValueError: If no data has been imported yet.

       Example:
           >>> result = await get_schema(ctx)
           >>> print(result["columns"])
           [{"name": "order_id", "type": "INTEGER", "nullable": false}, ...]
       """
       db = ctx.request_context.lifespan_context["db"]
       current_source = ctx.request_context.lifespan_context.get("current_source")

       if current_source is None:
           raise ValueError("No data imported. Use import_csv, import_excel, or import_database first.")

       await ctx.info("Retrieving schema for imported data")

       try:
           schema_rows = db.execute("DESCRIBE imported_data").fetchall()
       except Exception as e:
           raise ValueError(f"No data available: {e}")

       columns = [
           SchemaColumn(
               name=col[0],
               type=col[1],
               nullable=col[2] == "YES",
               warnings=[]
           ).model_dump()
           for col in schema_rows
       ]

       type_overrides = ctx.request_context.lifespan_context.get("type_overrides", {})

       # Apply type overrides to response
       for col in columns:
           if col["name"] in type_overrides:
               col["type_override"] = type_overrides[col["name"]]

       return {
           "columns": columns,
           "row_count": current_source.get("row_count", 0),
           "source_type": current_source.get("type", "unknown"),
           "type_overrides": type_overrides,
       }


   async def override_column_type(
       column_name: str,
       new_type: str,
       ctx: Context,
   ) -> dict:
       """Override the inferred type for a column.

       Use this when schema inference got the type wrong (e.g., order_id inferred
       as INTEGER but should be treated as VARCHAR to preserve leading zeros).

       The override is applied when querying data - the original data is unchanged.

       Args:
           column_name: Name of column to override
           new_type: New type to use (VARCHAR, INTEGER, DOUBLE, DATE, TIMESTAMP, BOOLEAN)

       Returns:
           Dictionary confirming the override.

       Example:
           >>> await override_column_type("order_id", "VARCHAR", ctx)
           {"column": "order_id", "original_type": "BIGINT", "new_type": "VARCHAR"}
       """
       db = ctx.request_context.lifespan_context["db"]

       await ctx.info(f"Overriding type for column {column_name} to {new_type}")

       # Validate column exists
       schema_rows = db.execute("DESCRIBE imported_data").fetchall()
       col_names = [row[0] for row in schema_rows]

       if column_name not in col_names:
           raise ValueError(
               f"Column '{column_name}' not found. Available columns: {col_names}"
           )

       # Get original type
       original_type = next(row[1] for row in schema_rows if row[0] == column_name)

       # Validate new type
       valid_types = {"VARCHAR", "INTEGER", "BIGINT", "DOUBLE", "DATE", "TIMESTAMP", "BOOLEAN"}
       if new_type.upper() not in valid_types:
           raise ValueError(
               f"Invalid type '{new_type}'. Valid types: {sorted(valid_types)}"
           )

       # Store override in lifespan context
       type_overrides = ctx.request_context.lifespan_context.setdefault("type_overrides", {})
       type_overrides[column_name] = new_type.upper()

       await ctx.info(f"Type override saved: {column_name} from {original_type} to {new_type}")

       return {
           "column": column_name,
           "original_type": original_type,
           "new_type": new_type.upper(),
       }
   ```

2. Update `src/mcp/data_source/tools/__init__.py` to export new tools.

3. Update `src/mcp/data_source/server.py` to register:
   ```python
   from .tools.schema_tools import get_schema, override_column_type

   mcp.tool()(get_schema)
   mcp.tool()(override_column_type)
   ```
  </action>
  <verify>
    - `python -c "from src.mcp.data_source.server import mcp; print([t.name for t in mcp.list_tools()])"` includes get_schema and override_column_type
  </verify>
  <done>Schema tools (get_schema, override_column_type) registered with server.</done>
</task>

<task type="auto">
  <name>Task 2: Query Tools</name>
  <files>
    src/mcp/data_source/tools/query_tools.py
    src/mcp/data_source/tools/__init__.py
    src/mcp/data_source/server.py
  </files>
  <action>
Create tools for querying imported data.

1. Create `src/mcp/data_source/tools/query_tools.py`:
   ```python
   """Data query tools for Data Source MCP."""

   from typing import Any
   from fastmcp import Context

   from ..models import RowData, QueryResult
   from ..utils import compute_row_checksum


   async def get_row(row_number: int, ctx: Context) -> dict:
       """Get a single row by its 1-based row number.

       Args:
           row_number: 1-based row number (1 = first data row)

       Returns:
           Dictionary with row data and checksum.

       Example:
           >>> result = await get_row(1, ctx)
           >>> print(result["data"])
           {"order_id": 1001, "customer": "John Doe", ...}
       """
       db = ctx.request_context.lifespan_context["db"]
       type_overrides = ctx.request_context.lifespan_context.get("type_overrides", {})

       if row_number < 1:
           raise ValueError("Row number must be >= 1")

       await ctx.info(f"Fetching row {row_number}")

       # Get column names
       schema = db.execute("DESCRIBE imported_data").fetchall()
       columns = [col[0] for col in schema]

       # Build SELECT with type casts for overrides
       select_parts = []
       for col in columns:
           if col in type_overrides:
               select_parts.append(f'CAST("{col}" AS {type_overrides[col]}) AS "{col}"')
           else:
               select_parts.append(f'"{col}"')

       select_clause = ", ".join(select_parts)

       # DuckDB uses 0-based OFFSET
       result = db.execute(f"""
           SELECT {select_clause} FROM imported_data
           LIMIT 1 OFFSET {row_number - 1}
       """).fetchone()

       if result is None:
           raise ValueError(f"Row {row_number} not found. Data may have fewer rows.")

       row_dict = dict(zip(columns, result))
       checksum = compute_row_checksum(row_dict)

       return RowData(
           row_number=row_number,
           data=row_dict,
           checksum=checksum
       ).model_dump()


   async def get_rows_by_filter(
       where_clause: str,
       ctx: Context,
       limit: int = 100,
       offset: int = 0,
   ) -> dict:
       """Get rows matching a SQL WHERE clause.

       Args:
           where_clause: SQL WHERE condition (without 'WHERE' keyword)
               Example: "state = 'CA' AND weight > 5"
           limit: Maximum rows to return (default: 100, max: 1000)
           offset: Number of rows to skip (for pagination)

       Returns:
           QueryResult with matching rows and checksums.

       Example:
           >>> result = await get_rows_by_filter("state = 'CA'", ctx, limit=50)
           >>> print(result["total_count"])
           150
       """
       db = ctx.request_context.lifespan_context["db"]
       type_overrides = ctx.request_context.lifespan_context.get("type_overrides", {})

       # Enforce limits
       limit = min(limit, 1000)
       if limit < 1:
           limit = 100

       await ctx.info(f"Querying rows with filter: {where_clause}")

       # Get column names
       schema = db.execute("DESCRIBE imported_data").fetchall()
       columns = [col[0] for col in schema]

       # Build SELECT with type casts
       select_parts = []
       for col in columns:
           if col in type_overrides:
               select_parts.append(f'CAST("{col}" AS {type_overrides[col]}) AS "{col}"')
           else:
               select_parts.append(f'"{col}"')

       select_clause = ", ".join(select_parts)

       # Get total count first
       total_count = db.execute(f"""
           SELECT COUNT(*) FROM imported_data
           WHERE {where_clause}
       """).fetchone()[0]

       # Get rows with ROW_NUMBER for consistent numbering
       results = db.execute(f"""
           WITH numbered AS (
               SELECT ROW_NUMBER() OVER () as _row_num, {select_clause}
               FROM imported_data
               WHERE {where_clause}
           )
           SELECT * FROM numbered
           LIMIT {limit} OFFSET {offset}
       """).fetchall()

       # Process results
       rows = []
       for row in results:
           row_num = row[0]
           row_data = dict(zip(columns, row[1:]))  # Skip _row_num
           checksum = compute_row_checksum(row_data)
           rows.append(RowData(
               row_number=row_num,
               data=row_data,
               checksum=checksum
           ).model_dump())

       await ctx.info(f"Found {total_count} matching rows, returning {len(rows)}")

       return QueryResult(
           rows=rows,
           total_count=total_count
       ).model_dump()


   async def query_data(
       sql: str,
       ctx: Context,
   ) -> dict:
       """Execute a custom SQL query against imported data.

       The query runs against 'imported_data' table. Only SELECT queries allowed.

       Args:
           sql: SQL SELECT query (must start with SELECT)

       Returns:
           Dictionary with columns and rows.

       Example:
           >>> result = await query_data("SELECT state, COUNT(*) as cnt FROM imported_data GROUP BY state", ctx)
       """
       db = ctx.request_context.lifespan_context["db"]

       # Security: Only allow SELECT
       sql_upper = sql.strip().upper()
       if not sql_upper.startswith("SELECT"):
           raise ValueError("Only SELECT queries are allowed")

       # Block dangerous keywords
       dangerous = ["DROP", "DELETE", "INSERT", "UPDATE", "ALTER", "CREATE", "TRUNCATE"]
       for keyword in dangerous:
           if keyword in sql_upper:
               raise ValueError(f"Query contains forbidden keyword: {keyword}")

       await ctx.info(f"Executing query: {sql[:100]}...")

       results = db.execute(sql).fetchall()
       columns = [desc[0] for desc in db.description]

       return {
           "columns": columns,
           "rows": [dict(zip(columns, row)) for row in results],
           "row_count": len(results),
       }
   ```

2. Update `src/mcp/data_source/tools/__init__.py` to export new tools.

3. Update `src/mcp/data_source/server.py` to register:
   ```python
   from .tools.query_tools import get_row, get_rows_by_filter, query_data

   mcp.tool()(get_row)
   mcp.tool()(get_rows_by_filter)
   mcp.tool()(query_data)
   ```
  </action>
  <verify>
    - `python -c "from src.mcp.data_source.server import mcp; print([t.name for t in mcp.list_tools()])"` includes get_row, get_rows_by_filter, query_data
  </verify>
  <done>Query tools (get_row, get_rows_by_filter, query_data) registered with server.</done>
</task>

<task type="auto">
  <name>Task 3: Checksum Tools</name>
  <files>
    src/mcp/data_source/tools/checksum_tools.py
    src/mcp/data_source/tools/__init__.py
    src/mcp/data_source/server.py
    tests/mcp/test_checksum.py
  </files>
  <action>
Create tools for computing and verifying row checksums.

1. Create `src/mcp/data_source/tools/checksum_tools.py`:
   ```python
   """Checksum tools for data integrity verification."""

   from fastmcp import Context

   from ..models import ChecksumResult
   from ..utils import compute_row_checksum


   async def compute_checksums(
       ctx: Context,
       start_row: int = 1,
       end_row: int | None = None,
   ) -> dict:
       """Compute SHA-256 checksums for rows in imported data.

       Checksums are deterministic - same row data always produces same checksum.
       Used for data integrity verification during batch processing.

       Args:
           start_row: First row to checksum (1-based, default: 1)
           end_row: Last row to checksum (default: all rows)

       Returns:
           Dictionary with list of row checksums.

       Example:
           >>> result = await compute_checksums(ctx, start_row=1, end_row=100)
           >>> print(result["checksums"][0])
           {"row_number": 1, "checksum": "a1b2c3d4..."}
       """
       db = ctx.request_context.lifespan_context["db"]

       await ctx.info(f"Computing checksums for rows {start_row} to {end_row or 'end'}")

       # Get column names
       schema = db.execute("DESCRIBE imported_data").fetchall()
       columns = [col[0] for col in schema]

       # Get total row count
       total_rows = db.execute("SELECT COUNT(*) FROM imported_data").fetchone()[0]

       if end_row is None:
           end_row = total_rows

       # Validate range
       if start_row < 1:
           start_row = 1
       if end_row > total_rows:
           end_row = total_rows
       if start_row > end_row:
           raise ValueError(f"start_row ({start_row}) cannot be greater than end_row ({end_row})")

       # Fetch rows in the range
       limit = end_row - start_row + 1
       offset = start_row - 1

       results = db.execute(f"""
           SELECT * FROM imported_data
           LIMIT {limit} OFFSET {offset}
       """).fetchall()

       checksums = []
       for i, row in enumerate(results):
           row_dict = dict(zip(columns, row))
           checksum = compute_row_checksum(row_dict)
           checksums.append(ChecksumResult(
               row_number=start_row + i,
               checksum=checksum
           ).model_dump())

       await ctx.info(f"Computed {len(checksums)} checksums")

       return {
           "checksums": checksums,
           "count": len(checksums),
           "start_row": start_row,
           "end_row": end_row,
       }


   async def verify_checksum(
       row_number: int,
       expected_checksum: str,
       ctx: Context,
   ) -> dict:
       """Verify that a row's current checksum matches an expected value.

       Used to detect if source data has changed since job creation.

       Args:
           row_number: 1-based row number
           expected_checksum: Expected SHA-256 checksum

       Returns:
           Dictionary with verification result.

       Example:
           >>> result = await verify_checksum(1, "a1b2c3d4...", ctx)
           >>> print(result["matches"])
           True
       """
       db = ctx.request_context.lifespan_context["db"]

       await ctx.info(f"Verifying checksum for row {row_number}")

       # Get column names
       schema = db.execute("DESCRIBE imported_data").fetchall()
       columns = [col[0] for col in schema]

       # Get the row
       result = db.execute(f"""
           SELECT * FROM imported_data
           LIMIT 1 OFFSET {row_number - 1}
       """).fetchone()

       if result is None:
           raise ValueError(f"Row {row_number} not found")

       row_dict = dict(zip(columns, result))
       actual_checksum = compute_row_checksum(row_dict)

       matches = actual_checksum == expected_checksum

       if not matches:
           await ctx.info(f"Checksum mismatch for row {row_number}")

       return {
           "row_number": row_number,
           "expected_checksum": expected_checksum,
           "actual_checksum": actual_checksum,
           "matches": matches,
       }
   ```

2. Update `src/mcp/data_source/tools/__init__.py` to export new tools.

3. Update `src/mcp/data_source/server.py` to register:
   ```python
   from .tools.checksum_tools import compute_checksums, verify_checksum

   mcp.tool()(compute_checksums)
   mcp.tool()(verify_checksum)
   ```

4. Create `tests/mcp/test_checksum.py`:
   ```python
   """Test checksum functionality."""

   import pytest
   from src.mcp.data_source.utils import compute_row_checksum


   def test_checksum_deterministic():
       """Same data produces same checksum."""
       data1 = {"a": 1, "b": 2, "c": "hello"}
       data2 = {"a": 1, "b": 2, "c": "hello"}

       assert compute_row_checksum(data1) == compute_row_checksum(data2)


   def test_checksum_order_independent():
       """Key order doesn't affect checksum."""
       data1 = {"a": 1, "b": 2, "c": 3}
       data2 = {"c": 3, "a": 1, "b": 2}

       assert compute_row_checksum(data1) == compute_row_checksum(data2)


   def test_checksum_different_data():
       """Different data produces different checksum."""
       data1 = {"a": 1, "b": 2}
       data2 = {"a": 1, "b": 3}

       assert compute_row_checksum(data1) != compute_row_checksum(data2)


   def test_checksum_format():
       """Checksum is 64-char hex string (SHA-256)."""
       data = {"test": "value"}
       checksum = compute_row_checksum(data)

       assert len(checksum) == 64
       assert all(c in "0123456789abcdef" for c in checksum)
   ```
  </action>
  <verify>
    - `pytest tests/mcp/test_checksum.py -v` passes all tests
    - `python -c "from src.mcp.data_source.server import mcp; print([t.name for t in mcp.list_tools()])"` includes compute_checksums and verify_checksum
  </verify>
  <done>Checksum tools (compute_checksums, verify_checksum) registered and tested.</done>
</task>

</tasks>

<verification>
Run these commands to verify plan completion:

```bash
cd /Users/matthewhans/Desktop/Programming/ShipAgent
source .venv/bin/activate

# Run all tests
pytest tests/mcp/ -v

# Verify all tools are registered
python -c "
from src.mcp.data_source.server import mcp
tools = mcp.list_tools()
tool_names = [t.name for t in tools]

expected = [
    'import_csv', 'import_excel', 'list_sheets',
    'import_database', 'list_tables',
    'get_schema', 'override_column_type',
    'get_row', 'get_rows_by_filter', 'query_data',
    'compute_checksums', 'verify_checksum'
]

for name in expected:
    assert name in tool_names, f'{name} not found in {tool_names}'

print(f'All {len(expected)} tools registered successfully')
"

# Test checksum determinism
python -c "
from src.mcp.data_source.utils import compute_row_checksum

# Same data, different order
d1 = {'name': 'Alice', 'age': 30, 'city': 'LA'}
d2 = {'city': 'LA', 'name': 'Alice', 'age': 30}
assert compute_row_checksum(d1) == compute_row_checksum(d2)

# Different data
d3 = {'name': 'Bob', 'age': 30, 'city': 'LA'}
assert compute_row_checksum(d1) != compute_row_checksum(d3)

print('Checksum determinism verified')
"
```
</verification>

<success_criteria>
1. get_schema returns column info and type overrides
2. override_column_type persists type override in session context
3. get_row returns single row with checksum
4. get_rows_by_filter applies WHERE clause and returns matching rows with checksums
5. query_data executes custom SELECT queries (with security restrictions)
6. compute_checksums generates SHA-256 checksums for row ranges
7. verify_checksum compares expected vs actual checksum
8. Checksums are deterministic (order-independent)
9. All 12 tools registered with MCP server
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-source-mcp/02-05-SUMMARY.md`
</output>
