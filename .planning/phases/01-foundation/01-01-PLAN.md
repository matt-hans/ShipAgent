---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/schema.sql
  - src/db/models.py
  - src/db/connection.py
  - src/db/__init__.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Database tables exist for jobs, job_rows, and audit_logs"
    - "SQLAlchemy models can be imported and used"
    - "Database connection can be established and queries executed"
  artifacts:
    - path: "src/db/schema.sql"
      provides: "Raw SQL schema for reference"
      contains: "CREATE TABLE jobs"
    - path: "src/db/models.py"
      provides: "SQLAlchemy ORM models"
      exports: ["Job", "JobRow", "AuditLog", "JobStatus", "RowStatus"]
    - path: "src/db/connection.py"
      provides: "Database connection management"
      exports: ["get_db", "init_db", "engine"]
  key_links:
    - from: "src/db/models.py"
      to: "src/db/connection.py"
      via: "imports engine for Base.metadata"
      pattern: "from.*connection.*import"
---

<objective>
Create the SQLite database infrastructure for ShipAgent including schema definition, SQLAlchemy ORM models, and connection management.

Purpose: Establish the persistence layer that all subsequent phases depend on for job state, audit logs, and crash recovery.

Output: Working database layer with Job, JobRow, and AuditLog models that can be queried via SQLAlchemy.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize Python project structure</name>
  <files>
    - pyproject.toml
    - src/__init__.py
    - src/db/__init__.py
  </files>
  <action>
Create the Python project structure with pyproject.toml:

1. Create `pyproject.toml` with:
   - name: shipagent
   - version: 0.1.0
   - python: ">=3.11"
   - dependencies: sqlalchemy>=2.0, aiosqlite, pydantic>=2.0, fastapi, uvicorn

2. Create `src/__init__.py` (empty, marks as package)

3. Create `src/db/__init__.py` that exports:
   - From models: Job, JobRow, AuditLog, JobStatus, RowStatus
   - From connection: get_db, init_db, engine

Use modern pyproject.toml format (not setup.py). Include dev dependencies: pytest, pytest-asyncio, ruff.
  </action>
  <verify>
    - `python -c "import toml; toml.loads(open('pyproject.toml').read())"` parses without error (or use tomllib in 3.11+)
    - Directory structure exists: src/, src/db/
  </verify>
  <done>
    - pyproject.toml exists with all dependencies listed
    - src/db/__init__.py exists and will export models once created
  </done>
</task>

<task type="auto">
  <name>Task 2: Create SQLite schema and SQLAlchemy models</name>
  <files>
    - src/db/schema.sql
    - src/db/models.py
  </files>
  <action>
Create the database schema based on Phase 1 CONTEXT.md requirements:

**src/db/schema.sql** (reference SQL for documentation):

```sql
-- Jobs table
CREATE TABLE jobs (
    id TEXT PRIMARY KEY,  -- UUID
    name TEXT NOT NULL,
    description TEXT,
    original_command TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending',  -- pending, running, paused, completed, failed, cancelled
    mode TEXT NOT NULL DEFAULT 'confirm',  -- confirm, auto

    -- Counts
    total_rows INTEGER NOT NULL DEFAULT 0,
    processed_rows INTEGER NOT NULL DEFAULT 0,
    successful_rows INTEGER NOT NULL DEFAULT 0,
    failed_rows INTEGER NOT NULL DEFAULT 0,

    -- Cost tracking
    total_cost_cents INTEGER,  -- Store as cents to avoid float issues

    -- Timestamps
    created_at TEXT NOT NULL,  -- ISO8601
    started_at TEXT,
    completed_at TEXT,
    updated_at TEXT NOT NULL,

    -- Error info (if failed)
    error_code TEXT,
    error_message TEXT
);

-- Job rows table (per-row tracking for retry)
CREATE TABLE job_rows (
    id TEXT PRIMARY KEY,  -- UUID
    job_id TEXT NOT NULL REFERENCES jobs(id) ON DELETE CASCADE,
    row_number INTEGER NOT NULL,
    row_checksum TEXT NOT NULL,  -- SHA-256 of row data
    status TEXT NOT NULL DEFAULT 'pending',  -- pending, processing, completed, failed, skipped

    -- Result data
    tracking_number TEXT,
    label_path TEXT,
    cost_cents INTEGER,

    -- Error info (if failed)
    error_code TEXT,
    error_message TEXT,

    -- Timestamps
    created_at TEXT NOT NULL,
    processed_at TEXT,

    UNIQUE(job_id, row_number)
);

-- Audit logs table
CREATE TABLE audit_logs (
    id TEXT PRIMARY KEY,  -- UUID
    job_id TEXT NOT NULL REFERENCES jobs(id) ON DELETE CASCADE,
    timestamp TEXT NOT NULL,  -- ISO8601
    level TEXT NOT NULL,  -- INFO, WARNING, ERROR
    event_type TEXT NOT NULL,  -- state_change, api_call, row_event, error
    message TEXT NOT NULL,

    -- Structured data (JSON)
    details TEXT,  -- JSON blob for request/response payloads

    -- Row context (optional)
    row_number INTEGER
);

-- Indexes
CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_created_at ON jobs(created_at);
CREATE INDEX idx_job_rows_job_id ON job_rows(job_id);
CREATE INDEX idx_job_rows_status ON job_rows(status);
CREATE INDEX idx_audit_logs_job_id ON audit_logs(job_id);
CREATE INDEX idx_audit_logs_timestamp ON audit_logs(timestamp);
```

**src/db/models.py**:

Create SQLAlchemy 2.0 models using declarative base:

1. **Enums** (using Python enum.Enum + SQLAlchemy Enum):
   - `JobStatus`: pending, running, paused, completed, failed, cancelled
   - `RowStatus`: pending, processing, completed, failed, skipped
   - `LogLevel`: INFO, WARNING, ERROR
   - `EventType`: state_change, api_call, row_event, error

2. **Job model**:
   - All fields from schema above
   - Relationship to job_rows (one-to-many)
   - Relationship to audit_logs (one-to-many)
   - Use `uuid.uuid4()` for default id
   - Use `datetime.utcnow().isoformat()` for timestamps

3. **JobRow model**:
   - All fields from schema
   - Foreign key to Job
   - Unique constraint on (job_id, row_number)

4. **AuditLog model**:
   - All fields from schema
   - Foreign key to Job
   - `details` as JSON-compatible TEXT (use SQLAlchemy's JSON type with SQLite fallback)

Use SQLAlchemy 2.0 style (Mapped, mapped_column). Store timestamps as ISO8601 strings for SQLite compatibility.
  </action>
  <verify>
    - `python -c "from src.db.models import Job, JobRow, AuditLog, JobStatus, RowStatus"` succeeds
    - All enum values accessible: `python -c "from src.db.models import JobStatus; print(list(JobStatus))"`
  </verify>
  <done>
    - schema.sql documents the full schema
    - models.py defines Job, JobRow, AuditLog with all fields
    - Enums defined for JobStatus, RowStatus, LogLevel, EventType
    - Relationships configured between Job and JobRow/AuditLog
  </done>
</task>

<task type="auto">
  <name>Task 3: Create database connection management</name>
  <files>
    - src/db/connection.py
  </files>
  <action>
Create database connection management for both sync and async access:

**src/db/connection.py**:

1. **Configuration**:
   - `DATABASE_URL` from environment variable, default to `sqlite:///./shipagent.db`
   - Support both sync (`sqlite:///`) and async (`sqlite+aiosqlite:///`) URLs

2. **Engine creation**:
   - Create sync `engine` using `create_engine()`
   - Create async `async_engine` using `create_async_engine()` with aiosqlite
   - Enable foreign keys for SQLite: `connect_args={"check_same_thread": False}` and pragma

3. **Session factories**:
   - `SessionLocal` - sync sessionmaker bound to engine
   - `AsyncSessionLocal` - async sessionmaker bound to async_engine

4. **Dependency functions**:
   - `get_db()` - Generator yielding sync session (for FastAPI Depends)
   - `get_async_db()` - Async generator yielding async session

5. **Initialization**:
   - `init_db()` - Creates all tables using Base.metadata.create_all()
   - `async_init_db()` - Async version for async contexts

Import `Base` from models.py (the declarative base).

SQLite pragma for foreign keys:
```python
from sqlalchemy import event

@event.listens_for(engine, "connect")
def set_sqlite_pragma(dbapi_connection, connection_record):
    cursor = dbapi_connection.cursor()
    cursor.execute("PRAGMA foreign_keys=ON")
    cursor.close()
```
  </action>
  <verify>
    - `python -c "from src.db.connection import engine, init_db; init_db()"` creates shipagent.db
    - `sqlite3 shipagent.db '.tables'` shows jobs, job_rows, audit_logs
    - `python -c "from src.db import Job, get_db"` imports work
  </verify>
  <done>
    - connection.py provides sync and async database access
    - init_db() creates all tables
    - get_db() works as FastAPI dependency
    - Foreign keys enforced in SQLite
    - Database file created at configured path
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Schema verification**:
   ```bash
   python -c "from src.db.connection import init_db; init_db()"
   sqlite3 shipagent.db '.schema jobs'
   sqlite3 shipagent.db '.schema job_rows'
   sqlite3 shipagent.db '.schema audit_logs'
   ```

2. **Model verification**:
   ```bash
   python -c "
   from src.db import Job, JobRow, AuditLog, JobStatus, get_db
   from src.db.connection import init_db
   init_db()

   # Create a test job
   db = next(get_db())
   job = Job(
       name='Test Job',
       original_command='test command',
       status=JobStatus.pending
   )
   db.add(job)
   db.commit()
   print(f'Created job: {job.id}')

   # Query it back
   found = db.query(Job).first()
   print(f'Found job: {found.name}, status: {found.status}')
   db.close()
   "
   ```

3. **Cleanup test database**:
   ```bash
   rm -f shipagent.db
   ```
</verification>

<success_criteria>
1. `from src.db import Job, JobRow, AuditLog, JobStatus, RowStatus, get_db, init_db` works
2. `init_db()` creates SQLite database with all three tables
3. Jobs can be created, queried, and updated via SQLAlchemy
4. Foreign key constraints enforced (deleting job cascades to rows and logs)
5. All enum values match CONTEXT.md specifications
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
