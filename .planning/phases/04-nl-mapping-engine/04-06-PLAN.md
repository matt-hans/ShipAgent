---
phase: 04-nl-mapping-engine
plan: 06
type: execute
wave: 3
depends_on: ["04-01", "04-02"]
files_modified:
  - src/orchestrator/nl_engine/elicitation.py
  - src/orchestrator/models/elicitation.py
  - tests/orchestrator/test_elicitation.py
autonomous: true

must_haves:
  truths:
    - "Ambiguous commands trigger elicitation with structured questions"
    - "User can select from options or provide free-text response"
    - "Elicitation supports date column, weight column, and dimension choices"
  artifacts:
    - path: "src/orchestrator/nl_engine/elicitation.py"
      provides: "create_elicitation_question, handle_elicitation_response functions"
      exports: ["create_elicitation_question", "handle_elicitation_response", "ELICITATION_TEMPLATES"]
    - path: "src/orchestrator/models/elicitation.py"
      provides: "ElicitationQuestion, ElicitationOption, ElicitationResponse models"
      exports: ["ElicitationQuestion", "ElicitationOption", "ElicitationResponse"]
  key_links:
    - from: "elicitation.py"
      to: "intent_parser.py needs_clarification"
      via: "clarification trigger"
      pattern: "needs_clarification.*True"
    - from: "elicitation.py"
      to: "filter_generator.py clarification_questions"
      via: "filter ambiguity"
      pattern: "clarification_questions"
---

<objective>
Implement MCP-style elicitation for handling ambiguous commands and missing information.

Purpose: When users issue ambiguous commands ("Ship the big ones") or when required information is missing, use structured questions to clarify rather than guessing. This follows the AskUserQuestion pattern from Claude Agent SDK per CONTEXT.md Decision 1.

Output: Elicitation models, question templates for common scenarios, and handlers for processing user responses.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-nl-mapping-engine/04-CONTEXT.md
@.planning/phases/04-nl-mapping-engine/04-RESEARCH.md
@.planning/phases/04-nl-mapping-engine/04-01-SUMMARY.md
@.planning/phases/04-nl-mapping-engine/04-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create elicitation models</name>
  <files>
    - src/orchestrator/models/elicitation.py
    - src/orchestrator/models/__init__.py
  </files>
  <action>
Create models for structured elicitation following Claude Agent SDK AskUserQuestion pattern.

**Reference:** https://platform.claude.com/docs/en/agent-sdk/user-input

**In elicitation.py, create:**

1. `ElicitationOption` - Pydantic model for a single option:
   - id: str  # Unique identifier (e.g., "order_date", "ship_by_date")
   - label: str  # Display label (e.g., "Order Date")
   - description: str  # Explanation (e.g., "When the order was placed")
   - value: Any = None  # Associated value if different from id

2. `ElicitationQuestion` - Pydantic model for a question:
   - id: str  # Unique question identifier
   - header: str  # Section header (e.g., "Date Column")
   - question: str  # The actual question text
   - options: list[ElicitationOption]  # Available choices
   - allow_free_text: bool = True  # Allow "Other" responses
   - multi_select: bool = False  # Allow multiple selections
   - required: bool = True  # Must be answered

3. `ElicitationResponse` - Pydantic model for user response:
   - question_id: str  # Which question was answered
   - selected_options: list[str] = []  # Selected option IDs
   - free_text: Optional[str] = None  # Custom response if any
   - timestamp: datetime

4. `ElicitationContext` - Pydantic model for full context:
   - questions: list[ElicitationQuestion]
   - responses: dict[str, ElicitationResponse] = {}  # question_id -> response
   - timeout_seconds: int = 60  # Per Agent SDK docs
   - complete: bool = False

**Model constraints:**
- Max 4 questions per elicitation (per Agent SDK)
- 2-4 options per question recommended
- All models use ConfigDict(from_attributes=True)

**Export from models/__init__.py.**
  </action>
  <verify>
```bash
python -c "
from src.orchestrator.models.elicitation import ElicitationQuestion, ElicitationOption, ElicitationResponse, ElicitationContext
q = ElicitationQuestion(
    id='date_col',
    header='Date Column',
    question='Which date column?',
    options=[ElicitationOption(id='order_date', label='Order Date', description='When placed')]
)
print(f'Question created: {q.header}')
"
```
  </verify>
  <done>ElicitationQuestion, ElicitationOption, ElicitationResponse, ElicitationContext models created matching Agent SDK pattern</done>
</task>

<task type="auto">
  <name>Task 2: Implement elicitation templates and handlers</name>
  <files>
    - src/orchestrator/nl_engine/elicitation.py
    - src/orchestrator/nl_engine/__init__.py
  </files>
  <action>
Create elicitation.py with common question templates and response handlers.

**Implement:**

1. `ELICITATION_TEMPLATES` - Dict of pre-built question templates:

   Per CONTEXT.md decisions, create templates for:

   a) `missing_date_column`:
   ```python
   ElicitationQuestion(
       id="date_column",
       header="Date Column",
       question="Which date column should I use for 'today's orders'?",
       options=[
           ElicitationOption(id="order_date", label="order_date", description="When the order was placed"),
           ElicitationOption(id="ship_by_date", label="ship_by_date", description="Required ship date"),
           ElicitationOption(id="created_at", label="created_at", description="Record creation timestamp")
       ]
   )
   ```

   b) `ambiguous_weight`:
   ```python
   ElicitationQuestion(
       id="weight_column",
       header="Weight",
       question="Which weight column should I use?",
       options=[
           ElicitationOption(id="package_weight", label="package_weight", description="Individual package weight"),
           ElicitationOption(id="total_weight", label="total_weight", description="Combined order weight")
       ]
   )
   ```

   c) `missing_dimensions`:
   ```python
   ElicitationQuestion(
       id="dimensions",
       header="Dimensions",
       question="Package dimensions are required. How would you like to provide them?",
       options=[
           ElicitationOption(id="default", label="Default", description="Use standard box: 10x10x10 in"),
           ElicitationOption(id="custom", label="Custom", description="Enter custom L x W x H"),
           ElicitationOption(id="add_column", label="Add Column", description="I'll add dimension columns to source")
       ]
   )
   ```

   d) `ambiguous_big`:
   ```python
   ElicitationQuestion(
       id="big_definition",
       header="Size Definition",
       question="What defines 'big'?",
       options=[
           ElicitationOption(id="weight", label="Weight", description="Weight > 5 lbs"),
           ElicitationOption(id="dimensions", label="Dimensions", description="Any dimension > 12 in"),
           ElicitationOption(id="value", label="Value", description="Order value > $100")
       ]
   )
   ```

   e) `missing_service`:
   ```python
   ElicitationQuestion(
       id="shipping_service",
       header="Shipping Service",
       question="Which shipping service should I use?",
       options=[
           ElicitationOption(id="ground", label="UPS Ground", description="3-5 business days"),
           ElicitationOption(id="2-day", label="2nd Day Air", description="2 business days"),
           ElicitationOption(id="overnight", label="Next Day Air", description="1 business day")
       ]
   )
   ```

2. `create_elicitation_question(template_id: str, schema: list[ColumnInfo] | None = None) -> ElicitationQuestion`:
   - Get template from ELICITATION_TEMPLATES
   - If schema provided, customize options based on actual columns
   - For date_column: Replace generic options with actual date columns from schema
   - For weight_column: Replace with actual numeric columns that look like weights
   - Return customized ElicitationQuestion

3. `handle_elicitation_response(response: ElicitationResponse, context: dict) -> dict`:
   - Process user's response
   - For date_column: Return {"date_column": selected_column_name}
   - For dimensions: If "custom", need to parse free_text for L x W x H
   - For weight_column: Return {"weight_column": selected_column_name}
   - Return dict of resolved values to merge into intent/filter

4. `needs_elicitation(intent: ShippingIntent | None = None, filter_result: SQLFilterResult | None = None) -> list[str]`:
   - Check if intent or filter needs clarification
   - Return list of template_ids for questions to ask
   - Examples:
     - intent.filter_criteria.needs_clarification -> check clarification_reason
     - filter_result.needs_clarification -> check clarification_questions
     - intent.service_code is None -> "missing_service"

5. `create_elicitation_context(template_ids: list[str], schema: list[ColumnInfo] | None = None) -> ElicitationContext`:
   - Build full context with all questions
   - Limit to max 4 questions per Agent SDK
   - Return ready-to-use context

**Export from nl_engine/__init__.py:**
- ELICITATION_TEMPLATES
- create_elicitation_question
- handle_elicitation_response
- needs_elicitation
- create_elicitation_context
  </action>
  <verify>
```bash
python -c "
from src.orchestrator.nl_engine.elicitation import ELICITATION_TEMPLATES, create_elicitation_question, needs_elicitation

# Check templates exist
print(f'Templates available: {list(ELICITATION_TEMPLATES.keys())}')

# Test question creation
q = create_elicitation_question('missing_date_column')
print(f'Question: {q.question}')
print(f'Options: {[o.label for o in q.options]}')
"
```
  </verify>
  <done>ELICITATION_TEMPLATES contains 5 common scenarios; create_elicitation_question customizes based on schema; needs_elicitation detects when clarification needed</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for elicitation</name>
  <files>
    - tests/orchestrator/test_elicitation.py
  </files>
  <action>
Create comprehensive tests for elicitation functionality.

**Test cases:**

1. `TestElicitationModels`:
   - test_option_minimal
   - test_option_with_value
   - test_question_with_options
   - test_question_multi_select
   - test_response_with_selection
   - test_response_with_free_text
   - test_context_with_questions

2. `TestElicitationTemplates`:
   - test_all_templates_exist -> 5 templates
   - test_missing_date_column_template
   - test_ambiguous_weight_template
   - test_missing_dimensions_template
   - test_ambiguous_big_template
   - test_missing_service_template

3. `TestCreateElicitationQuestion`:
   - test_returns_template_question
   - test_customizes_with_schema
   - test_date_columns_from_schema -> actual date columns in options
   - test_weight_columns_from_schema -> actual numeric columns in options
   - test_unknown_template_raises

4. `TestHandleElicitationResponse`:
   - test_date_column_response
   - test_weight_column_response
   - test_dimensions_default_response
   - test_dimensions_custom_response -> parses "10x12x8"
   - test_service_response

5. `TestNeedsElicitation`:
   - test_clear_intent_no_elicitation
   - test_ambiguous_filter_needs_elicitation
   - test_missing_service_needs_elicitation
   - test_multiple_issues_returns_all

6. `TestCreateElicitationContext`:
   - test_creates_context_with_questions
   - test_limits_to_four_questions
   - test_timeout_default_60

**Fixtures:**
- sample_schema_with_dates: Schema with order_date, ship_by_date
- sample_schema_with_weights: Schema with package_weight, total_weight
- ambiguous_intent: Intent with needs_clarification=True
- ambiguous_filter: SQLFilterResult with needs_clarification=True
  </action>
  <verify>
```bash
cd /Users/matthewhans/Desktop/Programming/ShipAgent && python -m pytest tests/orchestrator/test_elicitation.py -v
```
  </verify>
  <done>All elicitation tests pass; 5 templates covered; schema customization works</done>
</task>

</tasks>

<verification>
1. `python -c "from src.orchestrator.nl_engine.elicitation import ELICITATION_TEMPLATES"` succeeds
2. `python -c "from src.orchestrator.models.elicitation import ElicitationQuestion"` succeeds
3. `pytest tests/orchestrator/test_elicitation.py -v` passes
4. All 5 templates from CONTEXT.md are implemented
5. create_elicitation_question customizes options based on actual schema
</verification>

<success_criteria>
1. ElicitationQuestion/Option/Response models match Agent SDK pattern
2. 5 elicitation templates cover CONTEXT.md scenarios
3. create_elicitation_question customizes options from actual schema columns
4. handle_elicitation_response processes selections correctly
5. needs_elicitation detects clarification requirements
6. Unit tests pass (20+ tests)
</success_criteria>

<output>
After completion, create `.planning/phases/04-nl-mapping-engine/04-06-SUMMARY.md`
</output>
