---
phase: 06-batch-execution
plan: 07
type: execute
wave: 4
depends_on: ["06-01", "06-02", "06-03", "06-04", "06-05", "06-06"]
files_modified:
  - tests/integration/batch/test_batch_integration.py
  - tests/integration/batch/__init__.py
autonomous: true

must_haves:
  truths:
    - "End-to-end batch flow works: preview -> approve -> execute"
    - "Fail-fast halts on first UPS error"
    - "Crash recovery resumes from pending rows"
    - "Write-back persists tracking numbers to source"
    - "Mode switching works mid-session"
  artifacts:
    - path: "tests/integration/batch/test_batch_integration.py"
      provides: "Integration tests for complete batch flow"
      min_lines: 200
  key_links:
    - from: "tests/integration/batch/test_batch_integration.py"
      to: "all batch components"
      via: "End-to-end testing"
      pattern: "BatchExecutor|PreviewGenerator|write_back"
---

<objective>
Create integration tests verifying the complete batch execution flow.

Purpose: All batch components must work together: preview, mode switching, execution, fail-fast, crash recovery, and write-back. Integration tests verify these interactions.

Output: Comprehensive integration test suite for batch execution.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-batch-execution/06-CONTEXT.md
@.planning/phases/06-batch-execution/06-RESEARCH.md

# All batch components
@src/orchestrator/batch/__init__.py
@src/orchestrator/agent/tools.py
@src/mcp/data_source/tools/writeback_tools.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test infrastructure</name>
  <files>tests/integration/batch/__init__.py, tests/integration/batch/conftest.py</files>
  <action>
Create the integration test directory and fixtures.

`tests/integration/batch/__init__.py`:
```python
"""Integration tests for batch execution."""
```

`tests/integration/batch/conftest.py`:
```python
"""Fixtures for batch integration tests."""

import csv
import os
import tempfile
from pathlib import Path
from typing import Generator
from unittest.mock import AsyncMock

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.db.models import Base, Job, JobRow, JobStatus, RowStatus
from src.services.job_service import JobService
from src.services.audit_service import AuditService


@pytest.fixture
def temp_db() -> Generator[str, None, None]:
    """Create temporary SQLite database."""
    fd, path = tempfile.mkstemp(suffix=".db")
    os.close(fd)

    engine = create_engine(f"sqlite:///{path}")
    Base.metadata.create_all(engine)

    yield path

    os.unlink(path)


@pytest.fixture
def db_session(temp_db: str):
    """Create database session."""
    engine = create_engine(f"sqlite:///{temp_db}")
    Session = sessionmaker(bind=engine)
    session = Session()
    yield session
    session.close()


@pytest.fixture
def job_service(db_session) -> JobService:
    """Create JobService with test database."""
    return JobService(db_session)


@pytest.fixture
def audit_service(db_session) -> AuditService:
    """Create AuditService with test database."""
    return AuditService(db_session)


@pytest.fixture
def temp_csv() -> Generator[str, None, None]:
    """Create temporary CSV file with sample data."""
    fd, path = tempfile.mkstemp(suffix=".csv")

    with os.fdopen(fd, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=[
            "order_id", "recipient_name", "address", "city", "state", "zip", "weight"
        ])
        writer.writeheader()
        for i in range(10):
            writer.writerow({
                "order_id": 1000 + i,
                "recipient_name": f"Customer {i}",
                "address": f"{i}00 Main St",
                "city": "Los Angeles",
                "state": "CA",
                "zip": "90001",
                "weight": 2.5 + i * 0.5,
            })

    yield path
    os.unlink(path)


@pytest.fixture
def mock_data_mcp() -> AsyncMock:
    """Mock Data MCP call function."""
    mock = AsyncMock()

    async def call_impl(tool_name: str, args: dict):
        if tool_name == "get_rows_by_filter":
            # Return sample rows
            rows = [
                {"row_number": i, "data": {"order_id": 1000 + i, "recipient_name": f"Customer {i}"}}
                for i in range(min(args.get("limit", 10), 10))
            ]
            return {"rows": rows, "total_count": 10}
        elif tool_name == "get_row":
            return {"row_number": args["row_number"], "data": {"order_id": 1000 + args["row_number"]}}
        elif tool_name == "write_back":
            return {"success": True}
        return {}

    mock.side_effect = call_impl
    return mock


@pytest.fixture
def mock_ups_mcp() -> AsyncMock:
    """Mock UPS MCP call function."""
    mock = AsyncMock()

    async def call_impl(tool_name: str, args: dict):
        if tool_name == "rating_quote":
            return {"totalCharges": {"amount": "15.50"}}
        elif tool_name == "shipping_create":
            return {
                "trackingNumbers": ["1Z999AA10123456784"],
                "labelPaths": ["/labels/test.pdf"],
                "totalCharges": {"monetaryValue": "15.50"},
            }
        return {}

    mock.side_effect = call_impl
    return mock


@pytest.fixture
def sample_job(job_service: JobService) -> Job:
    """Create sample job with rows."""
    job = job_service.create_job(
        name="Test Batch",
        original_command="Ship California orders via Ground",
    )
    job_service.create_rows(job.id, [
        {"row_number": i, "row_checksum": f"hash{i}"}
        for i in range(1, 6)
    ])
    return job
```
  </action>
  <verify>
Run `python -c "from tests.integration.batch.conftest import *; print('OK')"`
  </verify>
  <done>
Integration test fixtures created for database, CSV files, and mock MCP calls.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create integration tests</name>
  <files>tests/integration/batch/test_batch_integration.py</files>
  <action>
Create comprehensive integration tests for the batch execution flow.

```python
"""Integration tests for batch execution flow.

Tests verify:
- End-to-end preview -> approve -> execute flow
- Fail-fast behavior on errors
- Crash recovery from interrupted jobs
- Write-back to data sources
- Mode switching behavior
"""

import pytest
from unittest.mock import AsyncMock, patch

from src.orchestrator.batch import (
    ExecutionMode,
    SessionModeManager,
    PreviewGenerator,
    BatchExecutor,
    check_interrupted_jobs,
    handle_recovery_choice,
    RecoveryChoice,
)
from src.db.models import JobStatus, RowStatus


class TestBatchPreviewFlow:
    """Tests for preview generation."""

    @pytest.mark.asyncio
    async def test_preview_generates_cost_estimates(
        self, job_service, audit_service, mock_data_mcp, mock_ups_mcp, sample_job
    ):
        """Preview should generate cost estimates for first 20 rows."""
        generator = PreviewGenerator(
            data_mcp_call=mock_data_mcp,
            ups_mcp_call=mock_ups_mcp,
        )

        preview = await generator.generate_preview(
            job_id=sample_job.id,
            filter_clause="1=1",
            mapping_template="{}",  # Simple template for test
            shipper_info={"name": "Test Shipper"},
        )

        assert preview.total_rows > 0
        assert len(preview.preview_rows) <= 20
        assert preview.total_estimated_cost_cents > 0

    @pytest.mark.asyncio
    async def test_preview_handles_large_batch(
        self, job_service, audit_service, mock_data_mcp, mock_ups_mcp, sample_job
    ):
        """Large batches should show aggregate estimate."""
        # Mock 50 total rows
        original_impl = mock_data_mcp.side_effect

        async def large_batch_impl(tool_name, args):
            if tool_name == "get_rows_by_filter":
                result = await original_impl(tool_name, args)
                result["total_count"] = 50
                return result
            return await original_impl(tool_name, args)

        mock_data_mcp.side_effect = large_batch_impl

        generator = PreviewGenerator(
            data_mcp_call=mock_data_mcp,
            ups_mcp_call=mock_ups_mcp,
        )

        preview = await generator.generate_preview(
            job_id=sample_job.id,
            filter_clause="1=1",
            mapping_template="{}",
            shipper_info={"name": "Test Shipper"},
        )

        assert preview.total_rows == 50
        assert preview.additional_rows == 50 - len(preview.preview_rows)


class TestBatchExecuteFlow:
    """Tests for batch execution."""

    @pytest.mark.asyncio
    async def test_execute_processes_all_rows(
        self, job_service, audit_service, mock_data_mcp, mock_ups_mcp, sample_job
    ):
        """Execute should process all pending rows."""
        executor = BatchExecutor(
            job_service=job_service,
            audit_service=audit_service,
            data_mcp_call=mock_data_mcp,
            ups_mcp_call=mock_ups_mcp,
        )

        result = await executor.execute(
            job_id=sample_job.id,
            mapping_template="{}",
            shipper_info={"name": "Test Shipper"},
        )

        assert result.success is True
        assert result.processed_rows == 5
        assert result.successful_rows == 5
        assert result.failed_rows == 0

        # Verify job status
        job = job_service.get_job(sample_job.id)
        assert job.status == JobStatus.completed.value

    @pytest.mark.asyncio
    async def test_execute_fail_fast(
        self, job_service, audit_service, mock_data_mcp, mock_ups_mcp, sample_job
    ):
        """Execution should halt on first error."""
        call_count = 0

        async def failing_ups_call(tool_name, args):
            nonlocal call_count
            call_count += 1
            if call_count == 3:  # Fail on third row
                raise Exception("UPS API Error")
            return {
                "trackingNumbers": ["1Z999AA10123456784"],
                "labelPaths": ["/labels/test.pdf"],
                "totalCharges": {"monetaryValue": "15.50"},
            }

        mock_ups_mcp.side_effect = failing_ups_call

        executor = BatchExecutor(
            job_service=job_service,
            audit_service=audit_service,
            data_mcp_call=mock_data_mcp,
            ups_mcp_call=mock_ups_mcp,
        )

        result = await executor.execute(
            job_id=sample_job.id,
            mapping_template="{}",
            shipper_info={"name": "Test Shipper"},
        )

        assert result.success is False
        assert result.processed_rows == 3  # Stopped at third row
        assert result.successful_rows == 2
        assert result.failed_rows == 1

        # Verify job status
        job = job_service.get_job(sample_job.id)
        assert job.status == JobStatus.failed.value

    @pytest.mark.asyncio
    async def test_execute_writes_back_tracking(
        self, job_service, audit_service, mock_data_mcp, mock_ups_mcp, sample_job
    ):
        """Execute should call write_back for each successful row."""
        executor = BatchExecutor(
            job_service=job_service,
            audit_service=audit_service,
            data_mcp_call=mock_data_mcp,
            ups_mcp_call=mock_ups_mcp,
        )

        await executor.execute(
            job_id=sample_job.id,
            mapping_template="{}",
            shipper_info={"name": "Test Shipper"},
        )

        # Check write_back was called for each row
        write_back_calls = [
            call for call in mock_data_mcp.call_args_list
            if call[0][0] == "write_back"
        ]
        assert len(write_back_calls) == 5


class TestCrashRecovery:
    """Tests for crash recovery."""

    def test_check_interrupted_finds_running_job(
        self, job_service, sample_job
    ):
        """Should detect jobs in running state."""
        # Simulate interrupted job
        job_service.update_status(sample_job.id, JobStatus.running)

        info = check_interrupted_jobs(job_service)

        assert info is not None
        assert info.job_id == sample_job.id

    def test_check_interrupted_no_running_jobs(self, job_service, sample_job):
        """Should return None if no interrupted jobs."""
        # Job stays in pending state
        info = check_interrupted_jobs(job_service)
        assert info is None

    @pytest.mark.asyncio
    async def test_resume_processes_only_pending(
        self, job_service, audit_service, mock_data_mcp, mock_ups_mcp, sample_job
    ):
        """Resume should skip completed rows."""
        # Simulate partial completion
        job_service.update_status(sample_job.id, JobStatus.running)
        rows = job_service.get_rows(sample_job.id)
        # Mark first 2 rows as completed
        job_service.complete_row(
            rows[0].id, "TRACK001", "/labels/1.pdf", 1550
        )
        job_service.complete_row(
            rows[1].id, "TRACK002", "/labels/2.pdf", 1550
        )

        executor = BatchExecutor(
            job_service=job_service,
            audit_service=audit_service,
            data_mcp_call=mock_data_mcp,
            ups_mcp_call=mock_ups_mcp,
        )

        result = await executor.execute(
            job_id=sample_job.id,
            mapping_template="{}",
            shipper_info={"name": "Test Shipper"},
        )

        # Should only process 3 remaining rows
        assert result.successful_rows == 3 + 2  # 3 new + 2 previously completed
        assert result.processed_rows == 5

    def test_handle_cancel_transitions_job(self, job_service, sample_job):
        """Cancel should transition job to cancelled."""
        job_service.update_status(sample_job.id, JobStatus.running)

        result = handle_recovery_choice(
            RecoveryChoice.CANCEL,
            sample_job.id,
            job_service,
        )

        assert result["action"] == "cancel"
        job = job_service.get_job(sample_job.id)
        assert job.status == JobStatus.cancelled.value


class TestModeSwitch:
    """Tests for execution mode switching."""

    def test_default_mode_is_confirm(self):
        """Session should default to confirm mode."""
        manager = SessionModeManager()
        assert manager.mode == ExecutionMode.CONFIRM

    def test_switch_to_auto(self):
        """Should be able to switch to auto mode."""
        manager = SessionModeManager()
        manager.set_mode(ExecutionMode.AUTO)
        assert manager.mode == ExecutionMode.AUTO

    def test_locked_mode_rejects_change(self):
        """Should not allow mode change during execution."""
        manager = SessionModeManager()
        manager.lock()

        with pytest.raises(ValueError):
            manager.set_mode(ExecutionMode.AUTO)

    def test_unlock_allows_change(self):
        """Should allow mode change after unlock."""
        manager = SessionModeManager()
        manager.lock()
        manager.unlock()
        manager.set_mode(ExecutionMode.AUTO)
        assert manager.mode == ExecutionMode.AUTO
```
  </action>
  <verify>
Run `pytest tests/integration/batch/ -v`
  </verify>
  <done>
All integration tests pass covering preview, execution, fail-fast, crash recovery, write-back, and mode switching.
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify requirement coverage</name>
  <files>tests/integration/batch/test_batch_integration.py</files>
  <action>
Add docstrings and comments mapping tests to requirements.

At the top of the test file, add:
```python
"""Integration tests for batch execution flow.

Requirement Coverage:
- BATCH-01: test_execute_processes_all_rows (processes 1-500+ shipments)
- BATCH-02: test_preview_generates_cost_estimates (preview with cost)
- BATCH-03: test_switch_to_auto (auto mode bypasses preview)
- BATCH-04: test_switch_to_auto, test_locked_mode_rejects_change (mode toggle)
- BATCH-05: test_execute_fail_fast (halts on first error)
- BATCH-06: test_resume_processes_only_pending (crash recovery)
- DATA-04: test_execute_writes_back_tracking (write-back)
"""
```

Run the full test suite to verify all tests pass and requirements are covered.
  </action>
  <verify>
Run `pytest tests/integration/batch/ -v --tb=short && echo "BATCH-01: Verified" && echo "BATCH-02: Verified" && echo "BATCH-03: Verified" && echo "BATCH-04: Verified" && echo "BATCH-05: Verified" && echo "BATCH-06: Verified" && echo "DATA-04: Verified"`
  </verify>
  <done>
All integration tests pass. Requirement coverage documented in test file.
  </done>
</task>

</tasks>

<verification>
- [ ] All integration tests pass
- [ ] Preview flow tested with cost estimates
- [ ] Execute flow tested end-to-end
- [ ] Fail-fast verified (halts on first error)
- [ ] Crash recovery verified (resumes from pending)
- [ ] Write-back verified (tracking numbers persisted)
- [ ] Mode switching verified (confirm/auto, locking)
- [ ] Requirement coverage documented
</verification>

<success_criteria>
- Integration tests verify complete batch workflow
- All 7 requirements (BATCH-01 through BATCH-06, DATA-04) have test coverage
- Tests use realistic fixtures with temp DB and files
- Mock MCP calls simulate real behavior
- Tests pass consistently
</success_criteria>

<output>
After completion, create `.planning/phases/06-batch-execution/06-07-SUMMARY.md`
</output>
