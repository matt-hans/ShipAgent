---
phase: 06-batch-execution
plan: 04
type: execute
wave: 2
depends_on: ["06-01", "06-02"]
files_modified:
  - src/orchestrator/batch/executor.py
  - src/orchestrator/batch/__init__.py
  - tests/unit/orchestrator/batch/test_executor.py
autonomous: true

must_haves:
  truths:
    - "BatchExecutor processes rows with per-row state commits"
    - "Execution halts on first error (fail-fast)"
    - "Pending rows from previous crash can be resumed"
    - "Tracking numbers written back to source after each row"
    - "Progress events emitted for each row"
  artifacts:
    - path: "src/orchestrator/batch/executor.py"
      provides: "BatchExecutor class with execution loop"
      min_lines: 200
      exports: ["BatchExecutor"]
    - path: "tests/unit/orchestrator/batch/test_executor.py"
      provides: "Unit tests for batch execution"
      min_lines: 150
  key_links:
    - from: "src/orchestrator/batch/executor.py"
      to: "src/services/job_service.py"
      via: "JobService state management"
      pattern: "job_service\\.(start_row|complete_row|fail_row)"
    - from: "src/orchestrator/batch/executor.py"
      to: "UPS MCP"
      via: "shipping_create tool calls"
      pattern: "shipping_create"
    - from: "src/orchestrator/batch/executor.py"
      to: "src/mcp/data_source/tools/writeback_tools.py"
      via: "write_back tool calls"
      pattern: "write_back"
---

<objective>
Implement the BatchExecutor for processing shipment batches with fail-fast and crash recovery.

Purpose: This is the core execution engine for BATCH-01 (batch processing), BATCH-05 (fail-fast), and BATCH-06 (crash recovery). It orchestrates the row-by-row processing loop with per-row state commits.

Output: BatchExecutor class that processes batches with full state tracking and error handling.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-batch-execution/06-CONTEXT.md
@.planning/phases/06-batch-execution/06-RESEARCH.md

# Dependencies
@src/orchestrator/batch/models.py
@src/orchestrator/batch/events.py
@src/services/job_service.py
@src/services/audit_service.py
@src/errors/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BatchExecutor class</name>
  <files>src/orchestrator/batch/executor.py</files>
  <action>
Create `src/orchestrator/batch/executor.py` with the core execution loop.

```python
"""Batch execution engine with fail-fast and crash recovery.

Per CONTEXT.md:
- Decision 3: Resume from first pending row on crash recovery
- Decision 4: Immediate write-back after each successful row

Per RESEARCH.md:
- Pattern 2: Per-row state checkpoint
- Pattern 4: Fail-fast execution loop
"""

from typing import Any, Callable, Awaitable
from jinja2 import Environment

from src.db.models import JobStatus, RowStatus
from src.services.job_service import JobService
from src.services.audit_service import AuditService, EventType
from src.errors.registry import translate_error
from src.orchestrator.batch.models import BatchResult
from src.orchestrator.batch.events import BatchEventEmitter
from src.orchestrator.filters.logistics import create_logistics_environment


class BatchExecutor:
    """Executes batch shipments with fail-fast and crash recovery."""

    def __init__(
        self,
        job_service: JobService,
        audit_service: AuditService,
        data_mcp_call: Callable[[str, dict], Awaitable[dict]],
        ups_mcp_call: Callable[[str, dict], Awaitable[dict]],
        jinja_env: Environment | None = None,
    ):
        """Initialize the batch executor.

        Args:
            job_service: JobService for state management
            audit_service: AuditService for audit logging
            data_mcp_call: Function to call Data MCP tools
            ups_mcp_call: Function to call UPS MCP tools
            jinja_env: Optional Jinja2 environment
        """
        self._job_service = job_service
        self._audit_service = audit_service
        self._data_mcp = data_mcp_call
        self._ups_mcp = ups_mcp_call
        self._jinja_env = jinja_env or create_logistics_environment()
        self._event_emitter = BatchEventEmitter()

    @property
    def events(self) -> BatchEventEmitter:
        """Get event emitter for observer registration."""
        return self._event_emitter

    async def execute(
        self,
        job_id: str,
        mapping_template: str,
        shipper_info: dict[str, Any],
    ) -> BatchResult:
        """Execute batch shipments with fail-fast behavior.

        Args:
            job_id: UUID of the job (already created with rows)
            mapping_template: Jinja2 template for payload generation
            shipper_info: Shipper address and account info

        Returns:
            BatchResult with execution outcome
        """
```

Implementation:

1. Get job and validate it exists
2. Transition job to running state (via job_service.update_status)
3. Log state change via audit_service
4. Emit batch_started event with total_rows

5. Get pending rows: `pending_rows = self._job_service.get_pending_rows(job_id)`
   - This supports crash recovery - skips already completed rows

6. For each job_row in pending_rows:
   a. Emit row_started event
   b. Get row data from Data MCP: `get_row(row_number)`
   c. Call `_process_single_row(job_id, job_row, row_data, mapping_template, shipper_info)`
   d. Emit row_completed event on success
   e. On exception: emit row_failed, re-raise for fail-fast

7. On success: transition to completed, emit batch_completed, return BatchResult(success=True)
8. On exception: set error, transition to failed, emit batch_failed, return BatchResult(success=False)

`_process_single_row` method:
1. job_service.start_row(job_row.id)
2. audit_service.log_row_event(job_id, row_number, "started")
3. Render template with row data
4. Call ups_mcp("shipping_create", payload)
5. Extract tracking_number, label_path, cost from result
6. job_service.complete_row(job_row.id, tracking_number, label_path, cost_cents)
7. Call data_mcp("write_back", {row_number, tracking_number}) - per CONTEXT.md Decision 4
8. audit_service.log_row_event(job_id, row_number, "completed", {tracking_number})
9. On exception: job_service.fail_row, audit_service.log_row_event "failed", re-raise

Helper methods:
- `_render_payload(template: str, row_data: dict, shipper_info: dict) -> dict`
- `_extract_shipment_result(ups_response: dict) -> tuple[str, str, int]` (tracking, label_path, cost_cents)
  </action>
  <verify>
Run `python -c "from src.orchestrator.batch.executor import BatchExecutor; print('OK')"`
  </verify>
  <done>
BatchExecutor class exists with execute method implementing fail-fast loop with per-row state commits.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update batch package exports</name>
  <files>src/orchestrator/batch/__init__.py</files>
  <action>
Update `src/orchestrator/batch/__init__.py` to export BatchExecutor.

Add import:
```python
from src.orchestrator.batch.executor import BatchExecutor
```

Add to __all__:
```python
__all__ = [
    # ... existing exports
    "BatchExecutor",
]
```
  </action>
  <verify>
Run `python -c "from src.orchestrator.batch import BatchExecutor; print('OK')"`
  </verify>
  <done>
BatchExecutor is exported from the batch package.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for BatchExecutor</name>
  <files>tests/unit/orchestrator/batch/test_executor.py</files>
  <action>
Create comprehensive unit tests for BatchExecutor.

Test setup:
- Mock JobService with in-memory state
- Mock AuditService
- Mock data_mcp_call and ups_mcp_call
- Create sample job with rows in pending state

Test cases:

1. `test_execute_success`:
   - 3 rows, all succeed
   - Verify all rows marked completed
   - Verify job status is completed
   - Verify BatchResult has correct counts

2. `test_execute_fail_fast`:
   - 3 rows, second fails
   - First row completed, second failed, third never processed
   - Job status is failed
   - BatchResult has error info

3. `test_execute_crash_recovery`:
   - 5 rows: 2 completed, 3 pending
   - Execute only processes pending rows
   - Completed rows not re-processed

4. `test_execute_state_commits`:
   - Verify start_row called before processing
   - Verify complete_row called after success
   - Verify fail_row called on error

5. `test_execute_write_back`:
   - Verify write_back called for each successful row
   - Verify tracking_number passed correctly

6. `test_execute_events_emitted`:
   - Add mock observer
   - Verify batch_started, row_started, row_completed, batch_completed called

7. `test_execute_audit_logging`:
   - Verify log_state_change called
   - Verify log_row_event called for each row

8. `test_execute_empty_batch`:
   - 0 pending rows
   - Should complete successfully with 0 processed

Use pytest fixtures and AsyncMock for async methods.
  </action>
  <verify>
Run `pytest tests/unit/orchestrator/batch/test_executor.py -v`
  </verify>
  <done>
All executor tests pass covering success, fail-fast, crash recovery, state commits, write-back, events, and audit logging.
  </done>
</task>

</tasks>

<verification>
- [ ] BatchExecutor imports successfully
- [ ] BatchExecutor exported from batch package
- [ ] execute method processes pending rows
- [ ] Fail-fast halts on first error
- [ ] Crash recovery skips completed rows
- [ ] State committed per-row (start_row, complete_row, fail_row)
- [ ] Write-back called after each successful row
- [ ] Events emitted for all lifecycle points
- [ ] Audit logging for all operations
- [ ] All unit tests pass
</verification>

<success_criteria>
- BatchExecutor processes batches with per-row state commits
- First error halts batch (fail-fast per BATCH-05)
- Resume works by processing only pending rows (BATCH-06)
- Write-back called immediately after each success (DATA-04)
- Events emitted for Phase 7 UI integration
- Unit tests provide comprehensive coverage
</success_criteria>

<output>
After completion, create `.planning/phases/06-batch-execution/06-04-SUMMARY.md`
</output>
